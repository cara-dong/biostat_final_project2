---
title: "Data Exploration"
author: "Manye Dong"
date: "2023-11-28"
output: github_document
---

## Goal: Predict the risk of death based on features 1-14
```{r message = FALSE, echo= FALSE}
library(tidyverse)
library(corrplot)
library(leaps)
library(glmnet)
library(caret)


options(
  ggplot2.continuous.colour = "viridis", 
  ggplots.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_disrete = scale_fill_viridis_d


```


```{r}
# import data and data cleaning
bc_data = read.csv("./Project_2_data.csv") |>
  janitor::clean_names() |> 
  na.omit()
```

### Data summary 
```{r}
# include a descriptive table with summary statistics for all variables

# continuous data
conti_var = c("age", "tumor_size", "regional_node_examined","reginol_node_positive", "survival_months")
bc_data |>
  select(all_of(conti_var)) |>
  summary() |>
  knitr::kable()

# discrete data count number of distinct variables


discre_var <- c("race", "marital_status", "t_stage", "n_stage", "x6th_stage", "differentiate", "grade", "a_stage", "estrogen_status", "progesterone_status", "status")

# Function to create a summary table for each variable
summary_table = function(variable) {
  counts = table(bc_data[[variable]])
  summary_df = data.frame(
    Variable = rep(variable, length(counts)),
    Value = paste(variable, names(counts), sep = "_"),
    Count = as.vector(counts)
  )
  return(summary_df)
}

summary_tables = lapply(discre_var, summary_table)
combined_summary = do.call(rbind, summary_tables) |>
  knitr::kable()
print(combined_summary)
```

### Outliers
```{r}
Q1 <- quantile(bc_data$survival_months, 0.25)
Q3 <- quantile(bc_data$survival_months, 0.75)
IQR <- Q3 - Q1

lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR

outliers <- bc_data |> filter((survival_months < lower_bound) | (survival_months > upper_bound))

bc_data <- anti_join(bc_data, outliers, by = c(colnames(bc_data)))
```

```{r}
colnames(bc_data)
```


### Survial Months distribution 
```{r}
# explore the distribution of the outcome and consider potential transformations if necessary
# look at the original distribution of survival months
hist(bc_data$survival_months, main = "Distribution of survival months", xlab = "Survival Month")

#try different transformation 
log_survival = log(bc_data$survival_months)
hist(log_survival, main = "Distribution of log_transformed survival months", xlab = "log-transformed survival months")

sqrt_survival = sqrt(bc_data$survival_months)
hist(sqrt_survival, main = "Distribution of sqrt(survival months)", xlab = "sqrt(survival months)")

sq_survival = (bc_data$survival_months^2)
hist(sq_survival, main = "Distribution of square(survival months)", xlab = "square(survival months)")

iv_survival = (1/bc_data$survival_months)
hist(iv_survival, main = "Distribution of inverse(survival months)", xlab = "inverse(survival months)", xlim = c(0,0.1),breaks=100)
```

### Convert categorical data to factor
```{r}
bc_data = 
  bc_data |>
  mutate(
    race = factor(race, labels = c("1", "2", "3"), levels = c("Black", "White", "Other")),
    marital_status = factor(marital_status, labels = c("1", "2", "3","4","5"),levels = c("Divorced", "Married", "Separated", "Single ", "Widowed")),
    t_stage = factor(t_stage, labels = c("1", "2", "3","4"),levels = c("T1", "T2", "T3", "T4")),
    n_stage = factor(n_stage, labels = c("1","2","3"),levels = c("N1","N2", "N3")),
    x6th_stage = factor(x6th_stage, labels = c("1", "2", "3","4","5"),levels = c("IIA","IIB","IIIA","IIIB","IIIC")),
    differentiate = factor(differentiate, labels = c("1", "2", "3","4"),levels = c("Moderately differentiated","Poorly differentiated","Undifferentiated","Well differentiated")),
    grade = factor(grade, labels = c("1", "2", "3","4"),levels = c("1","2","3"," anaplastic; Grade IV")),
    a_stage = factor(a_stage, labels = c("1","2"),levels = c("Distant","Regional")),
    estrogen_status = factor(estrogen_status, labels = c("0","1"),levels = c("Negative","Positive")),
    progesterone_status = factor(progesterone_status, labels = c("0","1"),levels = c("Negative","Positive")),
    status = factor(status, labels = c("0","1"),levels = c("Dead","Alive"))
    ) |> 
  rename(regional_node_positive = reginol_node_positive) 
```


## Look at data interaction and collinearity
```{r}
# Pairwise interaction and Correlation plot
bc_data |> 
  select(-status, -survival_months) |> 
  pairs()


cor_matrix <- 
  bc_data |> 
  select(-status, -survival_months) |> 
  mutate(across(where(is.factor), as.numeric)) |> 
  cor()

print(cor_matrix, digits = 3)

corrplot(cor_matrix, type = "upper", diag = FALSE, tl.cex = 0.5, tl.srt = 45)
```

```{r}
# boxplots for each variable
par(mfrow = c(2,3))

boxplot(bc_data$survival_months, main = "survival_months")
boxplot(bc_data$age, main = "age")
boxplot(bc_data$race, main = "race")
boxplot(bc_data$marital_status, main = "marital_status")
boxplot(bc_data$t_stage, main = "t_stage")
boxplot(bc_data$n_stage, main = "n_stage")

par(mfrow = c(2,4))
boxplot(bc_data$x6th_stage, main = "x6th_stage")
boxplot(bc_data$differentiate, main = "differentiate")
boxplot(bc_data$a_stage, main = "a_stage")
boxplot(bc_data$tumor_size, main = "tumor_size")
boxplot(bc_data$estrogen_status, main = "estrogen_status")
boxplot(bc_data$progesterone_status, main = "progesterone_status")
boxplot(bc_data$regional_node_examined, main = "regional_node_examined")
boxplot(bc_data$regional_node_positive, main = "regional_node_positive")
```

## Model and MLR selections
### MLR with all predictors

Create dummies:
```{r}
# Create a copy of the original data (optional step)
bc_data_dummy <- bc_data

# Specify the names of factor variables
factor_vars <- c("race", "marital_status", "t_stage", "n_stage", "x6th_stage", 
                 "differentiate", "grade", "a_stage", "estrogen_status", 
                 "progesterone_status", "status")

# Loop through each factor variable and create dummy variables
for (var in factor_vars) {
  # Convert the factor variable into dummy variables
  dummy_cols <- model.matrix(~ 0 + as.factor(bc_data[[var]]))
  
  # Modify column names to remove unwanted parts
  colnames(dummy_cols) <- gsub(paste0(var, "_bc_data\\[\\[", var, "\\]\\]"), "", colnames(dummy_cols))
  
  # Assign dummy variables to the dataframe
  colnames(dummy_cols) <- paste(var, colnames(dummy_cols), sep = "_")
  bc_data_dummy <- cbind(bc_data_dummy, dummy_cols)
}

# Remove original factor columns if desired
bc_data_dummy <- bc_data_dummy[, !names(bc_data_dummy) %in% factor_vars] |> janitor::clean_names()

head(bc_data_dummy)
```

```{r}
mult.fit = lm(survival_months ~ ., data = bc_data_dummy)

#logit_fit=glm(status ~ .-survival_months,family="binomial",data=bc_data)
#summary(logit_fit)

summary(mult.fit)
```
```{r}
# residual vs. leverage plot
plot(mult.fit, which = 4)

par(mfrow = c(2,2))
plot(mult.fit)
```

Looks like doesn't need a transformation on the outcome survival months.

Use box-cox transformation to double-check if we need to make transformations.
```{r message=FALSE, warning=FALSE}
# boxcox(mult.fit)
```

Since lambda approaches 1 and its 95% CI lies close to 1, it suggests that we do not need to make any transformation.

Based on Cook's distance, we will investigate the three influential points:
```{r}
view_influential = bc_data_dummy[c(1843, 1553, 1534), ]
view_influential
```

After investigation, it looks like these three points are not negatively impacting the results. So, we will not remove them. They have a reason to be there.


### MLR reducing multicollinearity using correlation matrix
First, find the highly correlated pairs:
```{r}
new_df = bc_data |> mutate(across(where(is.factor), as.numeric))
```

```{r}
cor_matrix = cor(new_df[, c(colnames(new_df))])

# Find the pairs where correlation is greater than or equal to 0.7 but less than 1
high_cor_pairs = which(cor_matrix >= 0.7 & cor_matrix < 1, arr.ind = TRUE)

# Extract the variable names for these pairs
high_cor_var_pairs = data.frame(
  Var1 = rownames(cor_matrix)[high_cor_pairs[, 1]],
  Var2 = colnames(cor_matrix)[high_cor_pairs[, 2]],
  Correlation = cor_matrix[high_cor_pairs]
)

high_cor_var_pairs |> 
  knitr::kable(digits=4)
```

Remove the one with lower correlation with outcome in every pair:
```{r}
cor_tumor = cor_matrix["survival_months", "tumor_size"]
cor_tstage = cor_matrix["survival_months", "t_stage"]

c(cor_tumor, cor_tstage)
```
Keep tumor_size.

```{r}
cor_tumor = cor_matrix["survival_months", "x6th_stage"]
cor_nstage = cor_matrix["survival_months", "n_stage"]
cor_regional = cor_matrix["survival_months", "regional_node_positive"]

c(cor_tumor, cor_nstage, cor_regional)
```
Keep tumor_size, delete the other two.

Fit the reduced model:
```{r}
reduced_df = bc_data_dummy |> select(-t_stage_as_factor_bc_data_var_1, -t_stage_as_factor_bc_data_var_2, -t_stage_as_factor_bc_data_var_3, -t_stage_as_factor_bc_data_var_4, -n_stage_as_factor_bc_data_var_1, -n_stage_as_factor_bc_data_var_2, -n_stage_as_factor_bc_data_var_3, -x6th_stage_as_factor_bc_data_var_1, -x6th_stage_as_factor_bc_data_var_2, -x6th_stage_as_factor_bc_data_var_3, -x6th_stage_as_factor_bc_data_var_4, -x6th_stage_as_factor_bc_data_var_5)
```

```{r}
reduced_model = lm(survival_months ~ ., data = reduced_df) 

summary(reduced_model) 
# |> broom::tidy() |> knitr::kable(digits=3)
```

Estrogen_status1 seems to be the most influential factor .

```{r}
plot(reduced_model)
```

Normality fit looks better at the left tail.

### Stepwise
Forward
```{r}
intercept_only = lm(survival_months ~ 1, data = bc_data_dummy)
step(intercept_only, direction = "forward", scope = formula(mult.fit), trace = FALSE)
#step_forward = MASS::stepAIC(mult.fit, direction = "forward", trace = FALSE) |>
  #broom::tidy()

#knitr::kable(step_forward, digits = 3)
```

```{r}
forward_pred = lm(survival_months ~ a_stage_as_factor_bc_data_var_1 + estrogen_status_as_factor_bc_data_var_1 + status_as_factor_bc_data_var_1 + x6th_stage_as_factor_bc_data_var_4 + tumor_size + differentiate_as_factor_bc_data_var_4 +  race_as_factor_bc_data_var_1, 
                  data = bc_data_dummy)

summary(forward_pred)
```

Backward
```{r}
step(mult.fit, direction = 'backward', trace = FALSE)
```

```{r}
backward_pred = lm(survival_months ~ t_stage_as_factor_bc_data_var_1 + t_stage_as_factor_bc_data_var_2 + n_stage_as_factor_bc_data_var_2 + a_stage_as_factor_bc_data_var_1 + estrogen_status_as_factor_bc_data_var_0 + status_as_factor_bc_data_var_0 + x6th_stage_as_factor_bc_data_var_3 + tumor_size + race_as_factor_bc_data_var_1, 
                  data = bc_data_dummy)

summary(backward_pred)
```

Both Directions
```{r}
step(mult.fit, direction = 'both', trace = FALSE)
```

```{r}
both_pred = lm(survival_months ~ t_stage_as_factor_bc_data_var_1 + t_stage_as_factor_bc_data_var_2 + n_stage_as_factor_bc_data_var_2 + a_stage_as_factor_bc_data_var_1 + estrogen_status_as_factor_bc_data_var_0 + status_as_factor_bc_data_var_0 + x6th_stage_as_factor_bc_data_var_3 + tumor_size + race_as_factor_bc_data_var_1+differentiate_as_factor_bc_data_var_4, 
                  data = bc_data_dummy)

summary(both_pred)
```

### LASSO
```{r}
# supply sequence of lambda values for the lasso cross validation for lambda
lambda_seq <- 10^seq(-3, 0, by = .1)
set.seed(2023)

# save matrix of predictors to pass to the lasso function
mat = makeX(bc_data[1:14])

response_dat.state =
  bc_data %>% 
  select(survival_months) %>% 
  as.matrix()

cv_lasso_fit <- glmnet::cv.glmnet(x = mat,
                                    y = response_dat.state,
                                    lambda = lambda_seq,
                                    nfolds = 10)
cv_lasso_fit
```
min lambda is 0.19.

```{r}
lasso_fit = glmnet::glmnet(x = mat,
                           y = response_dat.state,
                           lambda = cv_lasso_fit$lambda.min)
coef(lasso_fit)
```

Fit LASSO:
```{r}
pred_lasso = lm(survival_months ~ age+race_as_factor_bc_data_var_1+race_as_factor_bc_data_var_3+marital_status_as_factor_bc_data_var_2+marital_status_as_factor_bc_data_var_3+t_stage_as_factor_bc_data_var_1+t_stage_as_factor_bc_data_var_2+n_stage_as_factor_bc_data_var_1+n_stage_as_factor_bc_data_var_3+x6th_stage_as_factor_bc_data_var_1+differentiate_as_factor_bc_data_var_1+differentiate_as_factor_bc_data_var_2+grade_as_factor_bc_data_var_2+a_stage_as_factor_bc_data_var_1+a_stage_as_factor_bc_data_var_2+tumor_size+estrogen_status_as_factor_bc_data_var_0+progesterone_status_as_factor_bc_data_var_0+progesterone_status_as_factor_bc_data_var_1+status_as_factor_bc_data_var_0+status_as_factor_bc_data_var_1, data = bc_data_dummy)

# +status_as_factor_bc_data_var_0+status_as_factor_bc_data_var_1

summary(pred_lasso)
```

## The model we selected...

Choosing btw forward selection and stepwise selection based on adjusted R^2

Diagnostics for the two candidates:
```{r}
par(mfrow = c(2,2))
plot(forward_pred)
```

```{r}
par(mfrow = c(2,2))
plot(both_pred)
```

Still choose forward selection one, because less predictors used is better based on principle of parsimony.

## Chosen Model's Validation & Performance Evaluation

First, we will conduct a train-test split on bc_data_dummy:
```{r}
# train test split used for training and testing
# Set seed for reproducibility
set.seed(123)

# Split data into training and testing sets (80% train, 20% test)
train_indices <- sample(seq_len(nrow(bc_data_dummy)), 0.8 * nrow(bc_data_dummy))  # 80% train indices
train_data <- bc_data_dummy[train_indices, ]  # Training data
test_data <- bc_data_dummy[-train_indices, ]  # Testing data

# Separate predictor variables (train_x, test_x) and target variable (train_y, test_y)
train_x <- train_data[, -which(names(train_data) == "survival_months")]
train_y <- train_data$survival_months

test_x <- test_data[, -which(names(test_data) == "survival_months")]
test_y <- test_data$survival_months
```

### Test the Forward selection model
```{r}
# Fit the MLR model on the training data
mult.fit <- lm(survival_months ~ ., data = train_data)

# Predict on the test data
predicted_test <- predict(mult.fit, newdata = test_data)

# Calculate evaluation metrics (e.g., Mean Squared Error, R-squared)
rmse <- sqrt(mean((predicted_test - test_data$survival_months)^2))
rsquared <- summary(mult.fit)$r.squared

rsquared
```

```{r}
mlr_summary <- summary(mult.fit)
adjusted_r_squared_mlr <- mlr_summary$adj.r.squared
adjusted_r_squared_mlr
```

We will also cross-validate MLR to double-check:
```{r}
# Define the number of folds for cross-validation
num_folds <- 5  # You can adjust the number of folds as needed

# Define the control parameters for cross-validation
ctrl <- trainControl(method = "cv", number = num_folds)

# Train the MLR model with k-fold cross-validation
mult.fit_cv <- train(survival_months ~ ., data = bc_data, method = "lm", trControl = ctrl)

# Get cross-validated performance metrics
cv_results <- mult.fit_cv$results
print(cv_results)
```
The RMSE and R-squared appear similar.


## Additional: Logistic Regression
```{r}
# Fit logistic regression model
logistic_model <- glm(status ~ ., data = bc_data, family = "binomial")

# Summary of the logistic regression model
summary(logistic_model)

# Predict on the training set
predicted <- predict(logistic_model, type = "response")

# Display the predicted values
head(predicted)
```

Evaluate model performance on training set:
```{r}
library(caret)
library(pROC)

# Create the confusion matrix
conf_matrix <- confusionMatrix(as.factor(round(predicted)), as.factor(bc_data$status))

# Print the confusion matrix
print(conf_matrix)
```

```{r}
# ROC curve and AUC
roc_curve <- roc(bc_data$status, predicted)

# Plot ROC curve with x-axis as 1 - Specificity
plot(roc_curve, legacy.axes = TRUE, print.auc = TRUE, col = "pink", main = "ROC Curve for All Data")
```

This is a very good classification!

Use train-test split on logistic regression model to validate the model:
```{r}
# train test split used for training and testing
# Set seed for reproducibility
set.seed(123)

# Split data into training and testing sets (80% train, 20% test)
train_indices <- sample(seq_len(nrow(bc_data)), 0.8 * nrow(bc_data))  # 80% train indices
train_data <- bc_data[train_indices, ]  # Training data
test_data <- bc_data[-train_indices, ]  # Testing data

# Separate predictor variables (train_x, test_x) and target variable (train_y, test_y)
train_x <- train_data[, -which(names(train_data) == "status")]
train_y <- train_data$status

test_x <- test_data[, -which(names(test_data) == "status")]
test_y <- test_data$status
```

```{r}
# Fit logistic regression model using train data
logistic_validate <- glm(status ~ ., data = train_data, family = "binomial")

# Predict on test data
test_predictions <- predict(logistic_validate, newdata = test_data, type = "response")
```

```{r}
# Create the confusion matrix
conf_matrix_test <- confusionMatrix(as.factor(round(test_predictions)),
                                    as.factor(test_data$status))

# Print the confusion matrix
print(conf_matrix_test)
```

```{r}
# ROC curve and AUC
roc_curve_test <- roc(test_data$status, test_predictions)
# Plot ROC curve with x-axis as 1 - Specificity
plot(roc_curve_test, legacy.axes = TRUE, print.auc = TRUE, col = "darkgreen", main = "ROC Curve for Test Data")
```


## Additional/Optional: Compare performance for White vs Black groups
And can you improve the prediction performance gap btw these two groups for your model?

Use the forward model for white group only, then use the forward model for other group only
```{r}

```

Compare performance
```{r}

```

How to improve? - weighting the race variable?
```{r}
# Assign different weights based on the racial group
#bc_data$weight <- ifelse(bc_data$race == "White", 1, 2)  # Assign higher weight to "Black" or other minorities

# Define your linear regression model using lm()
#model <- lm(target_var ~ ., data = bc_data, weights = bc_data$weight)

# Fit the model
#fit <- summary(model)
```

